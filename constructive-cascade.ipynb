{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda1c856-3cab-42de-848a-c00a4e270720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045a1aad-0485-4c30-8a9b-4fcc0bcef8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    data = []\n",
    "    vehicle_ids = []\n",
    "\n",
    "    for i, file_name in enumerate(os.listdir(directory)):\n",
    "\n",
    "        if(i % 5000 == 0):\n",
    "            print(f'Loading {i}')\n",
    "            \n",
    "        # Extract vehicle IDs from the filename\n",
    "        vehicle_id = int(file_name.split('_')[0])\n",
    "        vehicle_ids.append(vehicle_id)\n",
    "\n",
    "        # Load NPY files\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        loaded_data = np.load(file_path)\n",
    "\n",
    "        data.append(loaded_data)\n",
    "        \n",
    "        # Reshape loaded data and append to a list\n",
    "        #reshaped_data = loaded_data.reshape((1, 32, 64))\n",
    "        # data.append(reshaped_data)\n",
    "\n",
    "    # Converting data and labels to PyTorch Tensors\n",
    "    data = np.array(data)\n",
    "    data = torch.from_numpy(data)\n",
    "    vehicle_ids = torch.tensor(vehicle_ids, dtype=torch.long)\n",
    "    \n",
    "    # Adjust labels to be in range 0 to 1361\n",
    "    if(torch.min(vehicle_ids).item() == 1):\n",
    "        vehicle_ids = vehicle_ids - 1\n",
    "    \n",
    "    return (data, vehicle_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "875e2b8f-817c-401e-be06-c5559b09c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 0\n",
      "Loading 5000\n",
      "Loading 10000\n",
      "Loading 15000\n",
      "Loading 20000\n",
      "Loading 25000\n",
      "Loading 30000\n",
      "Loading 35000\n",
      "Loading 40000\n",
      "Loading 45000\n",
      "Loading 0\n",
      "Loading 5000\n",
      "Loading 10000\n",
      "Loading 15000\n"
     ]
    }
   ],
   "source": [
    "# Load train data and labels from train directory\n",
    "train_directory = \"vehicle-x/train\"\n",
    "test_directory = \"vehicle-x/test\"\n",
    "\n",
    "train_data, vehicle_ids_train = load_data(train_directory)\n",
    "test_data, vehicle_ids_test = load_data(test_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96124e01-f8f5-42ac-b924-ab96a90b90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "test_batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "class CustomDataset():\n",
    "    def __init__(self, data_array, labels):\n",
    "        self.data = data_array\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create custom dataset from the tensors\n",
    "train_dataset = CustomDataset(train_data, vehicle_ids_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, vehicle_ids_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3c0331-c66b-4bbc-9a17-f15022e8efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CascadeNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): CascadeLayer(\n",
      "      (fc): Linear(in_features=2048, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=2112, out_features=1362, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the constructive cascade network\n",
    "class CascadeLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CascadeLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.fc(x))\n",
    "\n",
    "class CascadeNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, initial_hidden_size=64):\n",
    "        super(CascadeNN, self).__init__()\n",
    "        self.cascade_hidden_size = 32\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Initial layer\n",
    "        self.layers.append(CascadeLayer(input_size, initial_hidden_size))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(initial_hidden_size + self.input_size, output_size)\n",
    "\n",
    "    def add_cascade_layer(self):\n",
    "        # The new layer will take input from the last cascade layer and the original input\n",
    "        new_input_size = self.layers[-1].fc.out_features + self.layers[-1].fc.in_features\n",
    "        new_layer = CascadeLayer(new_input_size, self.cascade_hidden_size).to(device)\n",
    "        self.layers.append(new_layer)\n",
    "\n",
    "        # Adjust the output layer to accomodate the new cascade layer\n",
    "        self.output_layer = nn.Linear(self.output_layer.in_features + self.cascade_hidden_size, self.output_size).to(device)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = [x]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            combined_input = torch.cat(outputs, dim=1)\n",
    "            layer_output = layer(combined_input)\n",
    "            outputs.append(layer_output)\n",
    "        \n",
    "        final_combined_input = torch.cat(outputs, dim=1)\n",
    "        logits = self.output_layer(final_combined_input)\n",
    "        return nn.functional.log_softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialise network\n",
    "model = CascadeNN(2048, 1362).to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b55eaf-7d97-4626-a3b3-9b852eee6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "\n",
    "        pred = outputs.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(batch_labels.data.view_as(pred)).long().cpu().sum()\n",
    "        \n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], step[{batch_idx + 1}], loss: {loss.item():.4f}')\n",
    "\n",
    "    accuracy = correct / len(train_loader.dataset)\n",
    "    print(f'Train Accuracy {correct}/{len(train_loader.dataset)} ({accuracy:.4f})')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2de99dc-9a5f-4452-8255-c62eb5d3876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        outputs = model(batch_data)\n",
    "        \n",
    "        pred = outputs.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(batch_labels.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f'Test Accuracy {correct}/{len(train_loader.dataset)} ({accuracy:.4f})')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19019001-0a73-4e51-b643-cdd3c54bedce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], step[100], loss: 82.7862\n",
      "Epoch [1/100], step[200], loss: 87.4881\n",
      "Epoch [1/100], step[300], loss: 84.5644\n",
      "Epoch [1/100], step[400], loss: 85.9298\n",
      "Epoch [1/100], step[500], loss: 102.7145\n",
      "Epoch [1/100], step[600], loss: 100.9594\n",
      "Epoch [1/100], step[700], loss: 106.6409\n",
      "Epoch [1/100], step[800], loss: 90.3357\n",
      "Epoch [1/100], step[900], loss: 111.8907\n",
      "Epoch [1/100], step[1000], loss: 120.7866\n",
      "Epoch [1/100], step[1100], loss: 102.1017\n",
      "Epoch [1/100], step[1200], loss: 132.3623\n",
      "Epoch [1/100], step[1300], loss: 139.6849\n",
      "Epoch [1/100], step[1400], loss: 152.3833\n",
      "Train Accuracy 39/45438 (0.0009)\n",
      "Test Accuracy 34/45438 (0.0022)\n",
      "Epoch [2/100], step[100], loss: 145.5549\n",
      "Epoch [2/100], step[200], loss: 159.4249\n",
      "Epoch [2/100], step[300], loss: 157.5732\n",
      "Epoch [2/100], step[400], loss: 179.4848\n",
      "Epoch [2/100], step[500], loss: 198.4363\n",
      "Epoch [2/100], step[600], loss: 183.9865\n",
      "Epoch [2/100], step[700], loss: 222.1593\n",
      "Epoch [2/100], step[800], loss: 230.7349\n",
      "Epoch [2/100], step[900], loss: 278.7065\n",
      "Epoch [2/100], step[1000], loss: 267.1018\n",
      "Epoch [2/100], step[1100], loss: 257.9896\n",
      "Epoch [2/100], step[1200], loss: 254.8330\n",
      "Epoch [2/100], step[1300], loss: 309.0306\n",
      "Epoch [2/100], step[1400], loss: 257.7270\n",
      "Train Accuracy 1215/45438 (0.0267)\n",
      "Test Accuracy 56/45438 (0.0037)\n",
      "Epoch [3/100], step[100], loss: 270.7985\n",
      "Epoch [3/100], step[200], loss: 312.7998\n",
      "Epoch [3/100], step[300], loss: 287.3599\n",
      "Epoch [3/100], step[400], loss: 278.3065\n",
      "Epoch [3/100], step[500], loss: 352.2497\n",
      "Epoch [3/100], step[600], loss: 274.6200\n",
      "Epoch [3/100], step[700], loss: 250.5094\n",
      "Epoch [3/100], step[800], loss: 250.4344\n",
      "Epoch [3/100], step[900], loss: 312.6466\n",
      "Epoch [3/100], step[1000], loss: 418.4100\n",
      "Epoch [3/100], step[1100], loss: 272.6966\n",
      "Epoch [3/100], step[1200], loss: 353.7571\n",
      "Epoch [3/100], step[1300], loss: 339.5786\n",
      "Epoch [3/100], step[1400], loss: 374.1725\n",
      "Train Accuracy 1811/45438 (0.0399)\n",
      "Test Accuracy 77/45438 (0.0051)\n",
      "Epoch [4/100], step[100], loss: 262.6609\n",
      "Epoch [4/100], step[200], loss: 361.2160\n",
      "Epoch [4/100], step[300], loss: 337.0839\n",
      "Epoch [4/100], step[400], loss: 350.5085\n",
      "Epoch [4/100], step[500], loss: 324.9718\n",
      "Epoch [4/100], step[600], loss: 347.3723\n",
      "Epoch [4/100], step[700], loss: 296.3500\n",
      "Epoch [4/100], step[800], loss: 316.1975\n",
      "Epoch [4/100], step[900], loss: 369.4703\n",
      "Epoch [4/100], step[1000], loss: 411.4162\n",
      "Epoch [4/100], step[1100], loss: 354.6810\n",
      "Epoch [4/100], step[1200], loss: 330.2517\n",
      "Epoch [4/100], step[1300], loss: 293.0862\n",
      "Epoch [4/100], step[1400], loss: 292.6123\n",
      "Train Accuracy 2024/45438 (0.0445)\n",
      "Test Accuracy 82/45438 (0.0054)\n",
      "Epoch [5/100], step[100], loss: 318.7685\n",
      "Epoch [5/100], step[200], loss: 312.9993\n",
      "Epoch [5/100], step[300], loss: 356.1789\n",
      "Epoch [5/100], step[400], loss: 298.5366\n",
      "Epoch [5/100], step[500], loss: 367.8406\n",
      "Epoch [5/100], step[600], loss: 353.7978\n",
      "Epoch [5/100], step[700], loss: 392.3290\n",
      "Epoch [5/100], step[800], loss: 277.2616\n",
      "Epoch [5/100], step[900], loss: 416.2786\n",
      "Epoch [5/100], step[1000], loss: 410.5735\n",
      "Epoch [5/100], step[1100], loss: 386.7923\n",
      "Epoch [5/100], step[1200], loss: 349.9069\n",
      "Epoch [5/100], step[1300], loss: 432.7237\n",
      "Epoch [5/100], step[1400], loss: 382.2350\n",
      "Train Accuracy 2104/45438 (0.0463)\n",
      "Test Accuracy 73/45438 (0.0048)\n",
      "Epoch [6/100], step[100], loss: 344.1273\n",
      "Epoch [6/100], step[200], loss: 371.8279\n",
      "Epoch [6/100], step[300], loss: 376.1453\n",
      "Epoch [6/100], step[400], loss: 304.1529\n",
      "Epoch [6/100], step[500], loss: 339.0197\n",
      "Epoch [6/100], step[600], loss: 340.4474\n",
      "Epoch [6/100], step[700], loss: 390.8522\n",
      "Epoch [6/100], step[800], loss: 313.9142\n",
      "Epoch [6/100], step[900], loss: 346.5744\n",
      "Epoch [6/100], step[1000], loss: 438.4543\n",
      "Epoch [6/100], step[1100], loss: 370.3340\n",
      "Epoch [6/100], step[1200], loss: 367.8559\n",
      "Epoch [6/100], step[1300], loss: 348.9904\n",
      "Epoch [6/100], step[1400], loss: 346.4738\n",
      "Train Accuracy 2140/45438 (0.0471)\n",
      "Test Accuracy 79/45438 (0.0052)\n",
      "Epoch [7/100], step[100], loss: 348.0173\n",
      "Epoch [7/100], step[200], loss: 338.8867\n",
      "Epoch [7/100], step[300], loss: 398.0798\n",
      "Epoch [7/100], step[400], loss: 311.2768\n",
      "Epoch [7/100], step[500], loss: 326.6626\n",
      "Epoch [7/100], step[600], loss: 341.1747\n",
      "Epoch [7/100], step[700], loss: 354.0942\n",
      "Epoch [7/100], step[800], loss: 333.7390\n",
      "Epoch [7/100], step[900], loss: 280.1635\n",
      "Epoch [7/100], step[1000], loss: 319.0617\n",
      "Epoch [7/100], step[1100], loss: 416.8427\n",
      "Epoch [7/100], step[1200], loss: 289.0037\n",
      "Epoch [7/100], step[1300], loss: 364.3751\n",
      "Epoch [7/100], step[1400], loss: 410.3911\n",
      "Train Accuracy 2193/45438 (0.0483)\n",
      "Test Accuracy 70/45438 (0.0046)\n",
      "Epoch [8/100], step[100], loss: 344.7568\n",
      "Epoch [8/100], step[200], loss: 333.3162\n",
      "Epoch [8/100], step[300], loss: 333.3737\n",
      "Epoch [8/100], step[400], loss: 280.8262\n",
      "Epoch [8/100], step[500], loss: 340.0842\n",
      "Epoch [8/100], step[600], loss: 347.3877\n",
      "Epoch [8/100], step[700], loss: 350.9570\n",
      "Epoch [8/100], step[800], loss: 339.3754\n",
      "Epoch [8/100], step[900], loss: 288.5014\n",
      "Epoch [8/100], step[1000], loss: 374.1951\n",
      "Epoch [8/100], step[1100], loss: 398.8847\n",
      "Epoch [8/100], step[1200], loss: 395.4580\n",
      "Epoch [8/100], step[1300], loss: 317.9160\n",
      "Epoch [8/100], step[1400], loss: 334.7312\n",
      "Train Accuracy 2209/45438 (0.0486)\n",
      "Test Accuracy 91/45438 (0.0060)\n",
      "Epoch [9/100], step[100], loss: 336.2614\n",
      "Epoch [9/100], step[200], loss: 305.5632\n",
      "Epoch [9/100], step[300], loss: 423.6699\n",
      "Epoch [9/100], step[400], loss: 299.8991\n",
      "Epoch [9/100], step[500], loss: 326.4372\n",
      "Epoch [9/100], step[600], loss: 516.1003\n",
      "Epoch [9/100], step[700], loss: 352.7004\n",
      "Epoch [9/100], step[800], loss: 409.3917\n",
      "Epoch [9/100], step[900], loss: 269.5715\n",
      "Epoch [9/100], step[1000], loss: 317.3444\n",
      "Epoch [9/100], step[1100], loss: 294.1004\n",
      "Epoch [9/100], step[1200], loss: 272.3903\n",
      "Epoch [9/100], step[1300], loss: 334.0232\n",
      "Epoch [9/100], step[1400], loss: 344.7441\n",
      "Train Accuracy 2231/45438 (0.0491)\n",
      "Test Accuracy 86/45438 (0.0057)\n",
      "Epoch [10/100], step[100], loss: 294.3725\n",
      "Epoch [10/100], step[200], loss: 410.1059\n",
      "Epoch [10/100], step[300], loss: 418.9651\n",
      "Epoch [10/100], step[400], loss: 292.4481\n",
      "Epoch [10/100], step[500], loss: 363.3854\n",
      "Epoch [10/100], step[600], loss: 361.1297\n",
      "Epoch [10/100], step[700], loss: 296.9045\n",
      "Epoch [10/100], step[800], loss: 344.3220\n",
      "Epoch [10/100], step[900], loss: 358.6030\n",
      "Epoch [10/100], step[1000], loss: 316.3141\n",
      "Epoch [10/100], step[1100], loss: 329.4355\n",
      "Epoch [10/100], step[1200], loss: 432.3706\n",
      "Epoch [10/100], step[1300], loss: 329.9960\n",
      "Epoch [10/100], step[1400], loss: 325.2892\n",
      "Train Accuracy 2254/45438 (0.0496)\n",
      "Test Accuracy 77/45438 (0.0051)\n",
      "Epoch [11/100], step[100], loss: 470.0460\n",
      "Epoch [11/100], step[200], loss: 416.0951\n",
      "Epoch [11/100], step[300], loss: 311.2858\n",
      "Epoch [11/100], step[400], loss: 317.9545\n",
      "Epoch [11/100], step[500], loss: 354.6568\n",
      "Epoch [11/100], step[600], loss: 311.4817\n",
      "Epoch [11/100], step[700], loss: 325.7563\n",
      "Epoch [11/100], step[800], loss: 374.0643\n",
      "Epoch [11/100], step[900], loss: 321.9214\n",
      "Epoch [11/100], step[1000], loss: 345.0002\n",
      "Epoch [11/100], step[1100], loss: 223.5554\n",
      "Epoch [11/100], step[1200], loss: 338.4111\n",
      "Epoch [11/100], step[1300], loss: 405.2435\n",
      "Epoch [11/100], step[1400], loss: 311.0306\n",
      "Train Accuracy 2263/45438 (0.0498)\n",
      "Test Accuracy 92/45438 (0.0061)\n",
      "Epoch [12/100], step[100], loss: 285.6476\n",
      "Epoch [12/100], step[200], loss: 293.2117\n",
      "Epoch [12/100], step[300], loss: 350.4163\n",
      "Epoch [12/100], step[400], loss: 242.3807\n",
      "Epoch [12/100], step[500], loss: 386.0941\n",
      "Epoch [12/100], step[600], loss: 321.2844\n",
      "Epoch [12/100], step[700], loss: 374.2827\n",
      "Epoch [12/100], step[800], loss: 388.3296\n",
      "Epoch [12/100], step[900], loss: 319.9517\n",
      "Epoch [12/100], step[1000], loss: 307.2432\n",
      "Epoch [12/100], step[1100], loss: 309.2269\n",
      "Epoch [12/100], step[1200], loss: 372.1333\n",
      "Epoch [12/100], step[1300], loss: 337.7262\n",
      "Epoch [12/100], step[1400], loss: 351.0572\n",
      "Train Accuracy 2288/45438 (0.0504)\n",
      "Test Accuracy 96/45438 (0.0063)\n",
      "Epoch [13/100], step[100], loss: 325.6269\n",
      "Epoch [13/100], step[200], loss: 394.9997\n",
      "Epoch [13/100], step[300], loss: 354.2373\n",
      "Epoch [13/100], step[400], loss: 321.4535\n",
      "Epoch [13/100], step[500], loss: 392.2192\n",
      "Epoch [13/100], step[600], loss: 380.7112\n",
      "Epoch [13/100], step[700], loss: 311.2491\n",
      "Epoch [13/100], step[800], loss: 420.8624\n",
      "Epoch [13/100], step[900], loss: 344.2241\n",
      "Epoch [13/100], step[1000], loss: 372.0127\n",
      "Epoch [13/100], step[1100], loss: 345.8102\n",
      "Epoch [13/100], step[1200], loss: 358.4437\n",
      "Epoch [13/100], step[1300], loss: 345.1495\n",
      "Epoch [13/100], step[1400], loss: 363.1512\n",
      "Train Accuracy 2290/45438 (0.0504)\n",
      "Test Accuracy 101/45438 (0.0067)\n",
      "Epoch [14/100], step[100], loss: 256.0847\n",
      "Epoch [14/100], step[200], loss: 284.1167\n",
      "Epoch [14/100], step[300], loss: 360.9617\n",
      "Epoch [14/100], step[400], loss: 384.9206\n",
      "Epoch [14/100], step[500], loss: 366.3250\n",
      "Epoch [14/100], step[600], loss: 306.3390\n",
      "Epoch [14/100], step[700], loss: 322.6690\n",
      "Epoch [14/100], step[800], loss: 373.2541\n",
      "Epoch [14/100], step[900], loss: 350.9985\n",
      "Epoch [14/100], step[1000], loss: 353.4981\n",
      "Epoch [14/100], step[1100], loss: 404.9123\n",
      "Epoch [14/100], step[1200], loss: 352.4598\n",
      "Epoch [14/100], step[1300], loss: 317.8093\n",
      "Epoch [14/100], step[1400], loss: 315.0089\n",
      "Train Accuracy 2320/45438 (0.0511)\n",
      "Test Accuracy 92/45438 (0.0061)\n",
      "Epoch [15/100], step[100], loss: 415.0915\n",
      "Epoch [15/100], step[200], loss: 289.6623\n",
      "Epoch [15/100], step[300], loss: 320.6973\n",
      "Epoch [15/100], step[400], loss: 429.5361\n",
      "Epoch [15/100], step[500], loss: 355.3646\n",
      "Epoch [15/100], step[600], loss: 295.6248\n",
      "Epoch [15/100], step[700], loss: 372.0122\n",
      "Epoch [15/100], step[800], loss: 343.8985\n",
      "Epoch [15/100], step[900], loss: 404.6643\n",
      "Epoch [15/100], step[1000], loss: 311.4531\n",
      "Epoch [15/100], step[1100], loss: 402.7004\n",
      "Epoch [15/100], step[1200], loss: 395.2507\n",
      "Epoch [15/100], step[1300], loss: 257.8640\n",
      "Epoch [15/100], step[1400], loss: 338.7300\n",
      "Train Accuracy 2330/45438 (0.0513)\n",
      "Test Accuracy 89/45438 (0.0059)\n",
      "Epoch [16/100], step[100], loss: 387.7491\n",
      "Epoch [16/100], step[200], loss: 293.1722\n",
      "Epoch [16/100], step[300], loss: 356.6162\n",
      "Epoch [16/100], step[400], loss: 389.9377\n",
      "Epoch [16/100], step[500], loss: 293.0032\n",
      "Epoch [16/100], step[600], loss: 409.1799\n",
      "Epoch [16/100], step[700], loss: 326.6807\n",
      "Epoch [16/100], step[800], loss: 370.0629\n",
      "Epoch [16/100], step[900], loss: 335.4635\n",
      "Epoch [16/100], step[1000], loss: 411.3401\n",
      "Epoch [16/100], step[1100], loss: 394.5966\n",
      "Epoch [16/100], step[1200], loss: 285.8292\n",
      "Epoch [16/100], step[1300], loss: 315.0509\n",
      "Epoch [16/100], step[1400], loss: 394.5561\n",
      "Train Accuracy 2332/45438 (0.0513)\n",
      "Test Accuracy 91/45438 (0.0060)\n",
      "Epoch [17/100], step[100], loss: 321.1135\n",
      "Epoch [17/100], step[200], loss: 344.1853\n",
      "Epoch [17/100], step[300], loss: 348.7023\n",
      "Epoch [17/100], step[400], loss: 406.0301\n",
      "Epoch [17/100], step[500], loss: 318.4720\n",
      "Epoch [17/100], step[600], loss: 350.8444\n",
      "Epoch [17/100], step[700], loss: 283.2894\n",
      "Epoch [17/100], step[800], loss: 342.3441\n",
      "Epoch [17/100], step[900], loss: 383.4939\n",
      "Epoch [17/100], step[1000], loss: 358.3802\n",
      "Epoch [17/100], step[1100], loss: 310.5560\n",
      "Epoch [17/100], step[1200], loss: 329.1725\n",
      "Epoch [17/100], step[1300], loss: 382.0697\n",
      "Epoch [17/100], step[1400], loss: 442.5013\n",
      "Train Accuracy 2321/45438 (0.0511)\n",
      "Test Accuracy 92/45438 (0.0061)\n",
      "Epoch [18/100], step[100], loss: 341.2633\n",
      "Epoch [18/100], step[200], loss: 349.1086\n",
      "Epoch [18/100], step[300], loss: 355.2126\n",
      "Epoch [18/100], step[400], loss: 362.2003\n",
      "Epoch [18/100], step[500], loss: 344.3996\n",
      "Epoch [18/100], step[600], loss: 363.4925\n",
      "Epoch [18/100], step[700], loss: 313.6679\n",
      "Epoch [18/100], step[800], loss: 357.0159\n",
      "Epoch [18/100], step[900], loss: 265.4371\n",
      "Epoch [18/100], step[1000], loss: 467.7437\n",
      "Epoch [18/100], step[1100], loss: 307.3502\n",
      "Epoch [18/100], step[1200], loss: 364.3080\n",
      "Epoch [18/100], step[1300], loss: 278.1272\n",
      "Epoch [18/100], step[1400], loss: 291.0099\n",
      "Train Accuracy 2321/45438 (0.0511)\n",
      "Test Accuracy 90/45438 (0.0059)\n",
      "Epoch [19/100], step[100], loss: 349.2609\n",
      "Epoch [19/100], step[200], loss: 321.2376\n",
      "Epoch [19/100], step[300], loss: 295.1284\n",
      "Epoch [19/100], step[400], loss: 385.2888\n",
      "Epoch [19/100], step[500], loss: 310.1782\n",
      "Epoch [19/100], step[600], loss: 380.2404\n",
      "Epoch [19/100], step[700], loss: 323.7576\n",
      "Epoch [19/100], step[800], loss: 307.9109\n",
      "Epoch [19/100], step[900], loss: 348.6536\n",
      "Epoch [19/100], step[1000], loss: 323.9159\n",
      "Epoch [19/100], step[1100], loss: 318.5499\n",
      "Epoch [19/100], step[1200], loss: 337.5698\n",
      "Epoch [19/100], step[1300], loss: 308.6897\n",
      "Epoch [19/100], step[1400], loss: 272.6812\n",
      "Train Accuracy 2317/45438 (0.0510)\n",
      "Test Accuracy 89/45438 (0.0059)\n",
      "Epoch [20/100], step[100], loss: 289.8937\n",
      "Epoch [20/100], step[200], loss: 315.7264\n",
      "Epoch [20/100], step[300], loss: 288.3097\n",
      "Epoch [20/100], step[400], loss: 291.4156\n",
      "Epoch [20/100], step[500], loss: 336.7136\n",
      "Epoch [20/100], step[600], loss: 314.5544\n",
      "Epoch [20/100], step[700], loss: 326.1227\n",
      "Epoch [20/100], step[800], loss: 284.8550\n",
      "Epoch [20/100], step[900], loss: 269.5762\n",
      "Epoch [20/100], step[1000], loss: 384.7620\n",
      "Epoch [20/100], step[1100], loss: 352.3116\n",
      "Epoch [20/100], step[1200], loss: 256.2857\n",
      "Epoch [20/100], step[1300], loss: 329.9834\n",
      "Epoch [20/100], step[1400], loss: 306.1123\n",
      "Train Accuracy 2324/45438 (0.0511)\n",
      "Test Accuracy 88/45438 (0.0058)\n",
      "Epoch [21/100], step[100], loss: 294.8009\n",
      "Epoch [21/100], step[200], loss: 380.8377\n",
      "Epoch [21/100], step[300], loss: 285.3954\n",
      "Epoch [21/100], step[400], loss: 329.5977\n",
      "Epoch [21/100], step[500], loss: 347.4236\n",
      "Epoch [21/100], step[600], loss: 355.2516\n",
      "Epoch [21/100], step[700], loss: 391.5207\n",
      "Epoch [21/100], step[800], loss: 325.2705\n",
      "Epoch [21/100], step[900], loss: 368.7625\n",
      "Epoch [21/100], step[1000], loss: 282.0348\n",
      "Epoch [21/100], step[1100], loss: 320.3188\n",
      "Epoch [21/100], step[1200], loss: 282.3081\n",
      "Epoch [21/100], step[1300], loss: 348.4971\n",
      "Epoch [21/100], step[1400], loss: 334.5474\n",
      "Train Accuracy 2329/45438 (0.0513)\n",
      "Test Accuracy 89/45438 (0.0059)\n",
      "Performance hasn't improved for 5 epochs. Adding a new cascade layer.\n",
      "Epoch [22/100], step[100], loss: 7.3474\n",
      "Epoch [22/100], step[200], loss: 7.3263\n",
      "Epoch [22/100], step[300], loss: 7.2391\n",
      "Epoch [22/100], step[400], loss: 7.2178\n",
      "Epoch [22/100], step[500], loss: 7.2806\n",
      "Epoch [22/100], step[600], loss: 7.3290\n",
      "Epoch [22/100], step[700], loss: 7.2573\n",
      "Epoch [22/100], step[800], loss: 7.1494\n",
      "Epoch [22/100], step[900], loss: 7.2376\n",
      "Epoch [22/100], step[1000], loss: 7.2170\n",
      "Epoch [22/100], step[1100], loss: 7.3437\n",
      "Epoch [22/100], step[1200], loss: 7.2918\n",
      "Epoch [22/100], step[1300], loss: 7.3405\n",
      "Epoch [22/100], step[1400], loss: 7.2352\n",
      "Train Accuracy 33/45438 (0.0007)\n",
      "Test Accuracy 10/45438 (0.0007)\n",
      "Epoch [23/100], step[100], loss: 7.3035\n",
      "Epoch [23/100], step[200], loss: 7.2613\n",
      "Epoch [23/100], step[300], loss: 7.2598\n",
      "Epoch [23/100], step[400], loss: 7.2416\n",
      "Epoch [23/100], step[500], loss: 7.1898\n",
      "Epoch [23/100], step[600], loss: 7.2850\n",
      "Epoch [23/100], step[700], loss: 7.2949\n",
      "Epoch [23/100], step[800], loss: 7.1938\n",
      "Epoch [23/100], step[900], loss: 7.3240\n",
      "Epoch [23/100], step[1000], loss: 7.1686\n",
      "Epoch [23/100], step[1100], loss: 7.3311\n",
      "Epoch [23/100], step[1200], loss: 7.2346\n",
      "Epoch [23/100], step[1300], loss: 7.1902\n",
      "Epoch [23/100], step[1400], loss: 7.3017\n",
      "Train Accuracy 33/45438 (0.0007)\n",
      "Test Accuracy 10/45438 (0.0007)\n",
      "Epoch [24/100], step[100], loss: 7.2330\n",
      "Epoch [24/100], step[200], loss: 7.1516\n",
      "Epoch [24/100], step[300], loss: 7.1992\n",
      "Epoch [24/100], step[400], loss: 7.2706\n",
      "Epoch [24/100], step[500], loss: 7.3044\n",
      "Epoch [24/100], step[600], loss: 7.3201\n",
      "Epoch [24/100], step[700], loss: 7.3350\n",
      "Epoch [24/100], step[800], loss: 7.1777\n",
      "Epoch [24/100], step[900], loss: 7.1473\n",
      "Epoch [24/100], step[1000], loss: 7.2898\n",
      "Epoch [24/100], step[1100], loss: 7.2240\n",
      "Epoch [24/100], step[1200], loss: 7.2494\n",
      "Epoch [24/100], step[1300], loss: 7.2633\n",
      "Epoch [24/100], step[1400], loss: 7.1436\n",
      "Train Accuracy 33/45438 (0.0007)\n",
      "Test Accuracy 10/45438 (0.0007)\n",
      "Epoch [25/100], step[100], loss: 7.3683\n",
      "Epoch [25/100], step[200], loss: 7.2574\n",
      "Epoch [25/100], step[300], loss: 7.1716\n",
      "Epoch [25/100], step[400], loss: 7.3295\n",
      "Epoch [25/100], step[500], loss: 7.2802\n",
      "Epoch [25/100], step[600], loss: 7.1742\n",
      "Epoch [25/100], step[700], loss: 7.1806\n",
      "Epoch [25/100], step[800], loss: 7.3457\n",
      "Epoch [25/100], step[900], loss: 7.3110\n",
      "Epoch [25/100], step[1000], loss: 7.3065\n",
      "Epoch [25/100], step[1100], loss: 7.2028\n",
      "Epoch [25/100], step[1200], loss: 7.2513\n",
      "Epoch [25/100], step[1300], loss: 7.2274\n",
      "Epoch [25/100], step[1400], loss: 7.3516\n",
      "Train Accuracy 33/45438 (0.0007)\n",
      "Test Accuracy 10/45438 (0.0007)\n",
      "Epoch [26/100], step[100], loss: 7.3441\n",
      "Epoch [26/100], step[200], loss: 7.2863\n",
      "Epoch [26/100], step[300], loss: 7.2529\n",
      "Epoch [26/100], step[400], loss: 7.2692\n",
      "Epoch [26/100], step[500], loss: 7.2604\n",
      "Epoch [26/100], step[600], loss: 7.1846\n",
      "Epoch [26/100], step[700], loss: 7.1670\n",
      "Epoch [26/100], step[800], loss: 7.2439\n",
      "Epoch [26/100], step[900], loss: 7.2919\n",
      "Epoch [26/100], step[1000], loss: 7.2423\n",
      "Epoch [26/100], step[1100], loss: 7.2787\n",
      "Epoch [26/100], step[1200], loss: 7.2541\n",
      "Epoch [26/100], step[1300], loss: 7.2240\n",
      "Epoch [26/100], step[1400], loss: 7.2621\n",
      "Train Accuracy 33/45438 (0.0007)\n",
      "Test Accuracy 10/45438 (0.0007)\n",
      "Performance hasn't improved for 5 epochs. Adding a new cascade layer.\n",
      "Epoch [27/100], step[100], loss: 7.2451\n",
      "Epoch [27/100], step[200], loss: 7.2437\n",
      "Epoch [27/100], step[300], loss: 7.2300\n",
      "Epoch [27/100], step[400], loss: 7.3879\n",
      "Epoch [27/100], step[500], loss: 7.2408\n",
      "Epoch [27/100], step[600], loss: 7.2919\n",
      "Epoch [27/100], step[700], loss: 7.2397\n",
      "Epoch [27/100], step[800], loss: 7.2224\n",
      "Epoch [27/100], step[900], loss: 7.2207\n",
      "Epoch [27/100], step[1000], loss: 7.1542\n",
      "Epoch [27/100], step[1100], loss: 7.2507\n",
      "Epoch [27/100], step[1200], loss: 7.1722\n",
      "Epoch [27/100], step[1300], loss: 7.2603\n",
      "Epoch [27/100], step[1400], loss: 7.2933\n",
      "Train Accuracy 40/45438 (0.0009)\n",
      "Test Accuracy 13/45438 (0.0009)\n",
      "Epoch [28/100], step[100], loss: 7.2150\n",
      "Epoch [28/100], step[200], loss: 7.3128\n",
      "Epoch [28/100], step[300], loss: 7.2771\n",
      "Epoch [28/100], step[400], loss: 7.2116\n",
      "Epoch [28/100], step[500], loss: 7.2547\n",
      "Epoch [28/100], step[600], loss: 7.1793\n",
      "Epoch [28/100], step[700], loss: 7.2587\n",
      "Epoch [28/100], step[800], loss: 7.3400\n",
      "Epoch [28/100], step[900], loss: 7.1932\n",
      "Epoch [28/100], step[1000], loss: 7.2741\n",
      "Epoch [28/100], step[1100], loss: 7.2182\n",
      "Epoch [28/100], step[1200], loss: 7.2508\n",
      "Epoch [28/100], step[1300], loss: 7.2216\n",
      "Epoch [28/100], step[1400], loss: 7.2916\n",
      "Train Accuracy 40/45438 (0.0009)\n",
      "Test Accuracy 13/45438 (0.0009)\n",
      "Epoch [29/100], step[100], loss: 7.2661\n",
      "Epoch [29/100], step[200], loss: 7.2161\n",
      "Epoch [29/100], step[300], loss: 7.1603\n",
      "Epoch [29/100], step[400], loss: 7.2209\n",
      "Epoch [29/100], step[500], loss: 7.2552\n",
      "Epoch [29/100], step[600], loss: 7.3157\n",
      "Epoch [29/100], step[700], loss: 7.3031\n",
      "Epoch [29/100], step[800], loss: 7.1903\n",
      "Epoch [29/100], step[900], loss: 7.2314\n",
      "Epoch [29/100], step[1000], loss: 7.2251\n",
      "Epoch [29/100], step[1100], loss: 7.3767\n",
      "Epoch [29/100], step[1200], loss: 7.3361\n",
      "Epoch [29/100], step[1300], loss: 7.2434\n",
      "Epoch [29/100], step[1400], loss: 7.3209\n",
      "Train Accuracy 40/45438 (0.0009)\n",
      "Test Accuracy 13/45438 (0.0009)\n",
      "Epoch [30/100], step[100], loss: 7.2646\n",
      "Epoch [30/100], step[200], loss: 7.2911\n",
      "Epoch [30/100], step[300], loss: 7.3135\n",
      "Epoch [30/100], step[400], loss: 7.1932\n",
      "Epoch [30/100], step[500], loss: 7.2303\n",
      "Epoch [30/100], step[600], loss: 7.2552\n",
      "Epoch [30/100], step[700], loss: 7.2786\n",
      "Epoch [30/100], step[800], loss: 7.3351\n",
      "Epoch [30/100], step[900], loss: 7.2558\n",
      "Epoch [30/100], step[1000], loss: 7.2857\n",
      "Epoch [30/100], step[1100], loss: 7.3084\n",
      "Epoch [30/100], step[1200], loss: 7.2798\n",
      "Epoch [30/100], step[1300], loss: 7.2293\n",
      "Epoch [30/100], step[1400], loss: 7.1956\n",
      "Train Accuracy 40/45438 (0.0009)\n",
      "Test Accuracy 13/45438 (0.0009)\n",
      "Epoch [31/100], step[100], loss: 7.2262\n",
      "Epoch [31/100], step[200], loss: 7.2316\n",
      "Epoch [31/100], step[300], loss: 7.2354\n",
      "Epoch [31/100], step[400], loss: 7.2214\n",
      "Epoch [31/100], step[500], loss: 7.2181\n",
      "Epoch [31/100], step[600], loss: 7.2390\n",
      "Epoch [31/100], step[700], loss: 7.3872\n",
      "Epoch [31/100], step[800], loss: 7.3187\n",
      "Epoch [31/100], step[900], loss: 7.1392\n",
      "Epoch [31/100], step[1000], loss: 7.2320\n",
      "Epoch [31/100], step[1100], loss: 7.2786\n",
      "Epoch [31/100], step[1200], loss: 7.3801\n",
      "Epoch [31/100], step[1300], loss: 7.2571\n",
      "Epoch [31/100], step[1400], loss: 7.2299\n",
      "Train Accuracy 40/45438 (0.0009)\n",
      "Test Accuracy 13/45438 (0.0009)\n",
      "Performance hasn't improved for 5 epochs. Adding a new cascade layer.\n",
      "Epoch [32/100], step[100], loss: 7.2926\n",
      "Epoch [32/100], step[200], loss: 7.2479\n",
      "Epoch [32/100], step[300], loss: 7.3219\n",
      "Epoch [32/100], step[400], loss: 7.3077\n",
      "Epoch [32/100], step[500], loss: 7.2819\n",
      "Epoch [32/100], step[600], loss: 7.2900\n",
      "Epoch [32/100], step[700], loss: 7.3312\n",
      "Epoch [32/100], step[800], loss: 7.2445\n",
      "Epoch [32/100], step[900], loss: 7.2250\n",
      "Epoch [32/100], step[1000], loss: 7.2350\n",
      "Epoch [32/100], step[1100], loss: 7.2424\n",
      "Epoch [32/100], step[1200], loss: 7.3235\n",
      "Epoch [32/100], step[1300], loss: 7.2850\n",
      "Epoch [32/100], step[1400], loss: 7.2151\n",
      "Train Accuracy 35/45438 (0.0008)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [33/100], step[100], loss: 7.2605\n",
      "Epoch [33/100], step[200], loss: 7.1988\n",
      "Epoch [33/100], step[300], loss: 7.3990\n",
      "Epoch [33/100], step[400], loss: 7.2812\n",
      "Epoch [33/100], step[500], loss: 7.2142\n",
      "Epoch [33/100], step[600], loss: 7.3170\n",
      "Epoch [33/100], step[700], loss: 7.2239\n",
      "Epoch [33/100], step[800], loss: 7.2778\n",
      "Epoch [33/100], step[900], loss: 7.2358\n",
      "Epoch [33/100], step[1000], loss: 7.3010\n",
      "Epoch [33/100], step[1100], loss: 7.1738\n",
      "Epoch [33/100], step[1200], loss: 7.1983\n",
      "Epoch [33/100], step[1300], loss: 7.2255\n",
      "Epoch [33/100], step[1400], loss: 7.2475\n",
      "Train Accuracy 35/45438 (0.0008)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [34/100], step[100], loss: 7.3051\n",
      "Epoch [34/100], step[200], loss: 7.2878\n",
      "Epoch [34/100], step[300], loss: 7.2166\n",
      "Epoch [34/100], step[400], loss: 7.2383\n",
      "Epoch [34/100], step[500], loss: 7.2118\n",
      "Epoch [34/100], step[600], loss: 7.2958\n",
      "Epoch [34/100], step[700], loss: 7.2083\n",
      "Epoch [34/100], step[800], loss: 7.2629\n",
      "Epoch [34/100], step[900], loss: 7.1919\n",
      "Epoch [34/100], step[1000], loss: 7.3708\n",
      "Epoch [34/100], step[1100], loss: 7.2680\n",
      "Epoch [34/100], step[1200], loss: 7.1870\n",
      "Epoch [34/100], step[1300], loss: 7.3483\n",
      "Epoch [34/100], step[1400], loss: 7.2960\n",
      "Train Accuracy 35/45438 (0.0008)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [35/100], step[100], loss: 7.1913\n",
      "Epoch [35/100], step[200], loss: 7.2419\n",
      "Epoch [35/100], step[300], loss: 7.3057\n",
      "Epoch [35/100], step[400], loss: 7.2608\n",
      "Epoch [35/100], step[500], loss: 7.3278\n",
      "Epoch [35/100], step[600], loss: 7.2443\n",
      "Epoch [35/100], step[700], loss: 7.2974\n",
      "Epoch [35/100], step[800], loss: 7.4025\n",
      "Epoch [35/100], step[900], loss: 7.2870\n",
      "Epoch [35/100], step[1000], loss: 7.3567\n",
      "Epoch [35/100], step[1100], loss: 7.2681\n",
      "Epoch [35/100], step[1200], loss: 7.2282\n",
      "Epoch [35/100], step[1300], loss: 7.1918\n",
      "Epoch [35/100], step[1400], loss: 7.3142\n",
      "Train Accuracy 35/45438 (0.0008)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [36/100], step[100], loss: 7.2757\n",
      "Epoch [36/100], step[200], loss: 7.3146\n",
      "Epoch [36/100], step[300], loss: 7.3454\n",
      "Epoch [36/100], step[400], loss: 7.1896\n",
      "Epoch [36/100], step[500], loss: 7.2352\n",
      "Epoch [36/100], step[600], loss: 7.2875\n",
      "Epoch [36/100], step[700], loss: 7.3266\n",
      "Epoch [36/100], step[800], loss: 7.3031\n",
      "Epoch [36/100], step[900], loss: 7.3346\n",
      "Epoch [36/100], step[1000], loss: 7.2562\n",
      "Epoch [36/100], step[1100], loss: 7.2028\n",
      "Epoch [36/100], step[1200], loss: 7.2926\n",
      "Epoch [36/100], step[1300], loss: 7.2734\n",
      "Epoch [36/100], step[1400], loss: 7.1892\n",
      "Train Accuracy 35/45438 (0.0008)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Performance hasn't improved for 5 epochs. Adding a new cascade layer.\n",
      "Epoch [37/100], step[100], loss: 7.3336\n",
      "Epoch [37/100], step[200], loss: 7.2476\n",
      "Epoch [37/100], step[300], loss: 7.3061\n",
      "Epoch [37/100], step[400], loss: 7.1859\n",
      "Epoch [37/100], step[500], loss: 7.3195\n",
      "Epoch [37/100], step[600], loss: 7.2853\n",
      "Epoch [37/100], step[700], loss: 7.3676\n",
      "Epoch [37/100], step[800], loss: 7.2030\n",
      "Epoch [37/100], step[900], loss: 7.2574\n",
      "Epoch [37/100], step[1000], loss: 7.2738\n",
      "Epoch [37/100], step[1100], loss: 7.2864\n",
      "Epoch [37/100], step[1200], loss: 7.3065\n",
      "Epoch [37/100], step[1300], loss: 7.2816\n",
      "Epoch [37/100], step[1400], loss: 7.2799\n",
      "Train Accuracy 54/45438 (0.0012)\n",
      "Test Accuracy 14/45438 (0.0009)\n",
      "Epoch [38/100], step[100], loss: 7.2640\n",
      "Epoch [38/100], step[200], loss: 7.2774\n",
      "Epoch [38/100], step[300], loss: 7.2321\n",
      "Epoch [38/100], step[400], loss: 7.2167\n",
      "Epoch [38/100], step[500], loss: 7.2799\n",
      "Epoch [38/100], step[600], loss: 7.2066\n",
      "Epoch [38/100], step[700], loss: 7.2491\n",
      "Epoch [38/100], step[800], loss: 7.3181\n",
      "Epoch [38/100], step[900], loss: 7.2346\n",
      "Epoch [38/100], step[1000], loss: 7.3672\n",
      "Epoch [38/100], step[1100], loss: 7.2288\n",
      "Epoch [38/100], step[1200], loss: 7.2301\n",
      "Epoch [38/100], step[1300], loss: 7.3542\n",
      "Epoch [38/100], step[1400], loss: 7.2361\n",
      "Train Accuracy 54/45438 (0.0012)\n",
      "Test Accuracy 14/45438 (0.0009)\n",
      "Epoch [39/100], step[100], loss: 7.2731\n",
      "Epoch [39/100], step[200], loss: 7.2236\n",
      "Epoch [39/100], step[300], loss: 7.2700\n",
      "Epoch [39/100], step[400], loss: 7.2404\n",
      "Epoch [39/100], step[500], loss: 7.1771\n",
      "Epoch [39/100], step[600], loss: 7.3730\n",
      "Epoch [39/100], step[700], loss: 7.2432\n",
      "Epoch [39/100], step[800], loss: 7.1522\n",
      "Epoch [39/100], step[900], loss: 7.2368\n",
      "Epoch [39/100], step[1000], loss: 7.3036\n",
      "Epoch [39/100], step[1100], loss: 7.2198\n",
      "Epoch [39/100], step[1200], loss: 7.3182\n",
      "Epoch [39/100], step[1300], loss: 7.2557\n",
      "Epoch [39/100], step[1400], loss: 7.2841\n",
      "Train Accuracy 54/45438 (0.0012)\n",
      "Test Accuracy 14/45438 (0.0009)\n",
      "Epoch [40/100], step[100], loss: 7.2476\n",
      "Epoch [40/100], step[200], loss: 7.2086\n",
      "Epoch [40/100], step[300], loss: 7.3370\n",
      "Epoch [40/100], step[400], loss: 7.4101\n",
      "Epoch [40/100], step[500], loss: 7.2713\n",
      "Epoch [40/100], step[600], loss: 7.3235\n",
      "Epoch [40/100], step[700], loss: 7.2083\n",
      "Epoch [40/100], step[800], loss: 7.3374\n",
      "Epoch [40/100], step[900], loss: 7.2189\n",
      "Epoch [40/100], step[1000], loss: 7.2768\n",
      "Epoch [40/100], step[1100], loss: 7.3124\n",
      "Epoch [40/100], step[1200], loss: 7.2359\n",
      "Epoch [40/100], step[1300], loss: 7.2466\n",
      "Epoch [40/100], step[1400], loss: 7.3434\n",
      "Train Accuracy 54/45438 (0.0012)\n",
      "Test Accuracy 14/45438 (0.0009)\n",
      "Epoch [41/100], step[100], loss: 7.2049\n",
      "Epoch [41/100], step[200], loss: 7.1852\n",
      "Epoch [41/100], step[300], loss: 7.3010\n",
      "Epoch [41/100], step[400], loss: 7.2898\n",
      "Epoch [41/100], step[500], loss: 7.3031\n",
      "Epoch [41/100], step[600], loss: 7.3644\n",
      "Epoch [41/100], step[700], loss: 7.3316\n",
      "Epoch [41/100], step[800], loss: 7.2651\n",
      "Epoch [41/100], step[900], loss: 7.2416\n",
      "Epoch [41/100], step[1000], loss: 7.2411\n",
      "Epoch [41/100], step[1100], loss: 7.3006\n",
      "Epoch [41/100], step[1200], loss: 7.2078\n",
      "Epoch [41/100], step[1300], loss: 7.3211\n",
      "Epoch [41/100], step[1400], loss: 7.3446\n",
      "Train Accuracy 54/45438 (0.0012)\n",
      "Test Accuracy 14/45438 (0.0009)\n",
      "Performance hasn't improved for 5 epochs. Adding a new cascade layer.\n",
      "Epoch [42/100], step[100], loss: 7.3423\n",
      "Epoch [42/100], step[200], loss: 7.2308\n",
      "Epoch [42/100], step[300], loss: 7.2609\n",
      "Epoch [42/100], step[400], loss: 7.2884\n",
      "Epoch [42/100], step[500], loss: 7.2073\n",
      "Epoch [42/100], step[600], loss: 7.3311\n",
      "Epoch [42/100], step[700], loss: 7.3083\n",
      "Epoch [42/100], step[800], loss: 7.2804\n",
      "Epoch [42/100], step[900], loss: 7.2868\n",
      "Epoch [42/100], step[1000], loss: 7.3012\n",
      "Epoch [42/100], step[1100], loss: 7.3448\n",
      "Epoch [42/100], step[1200], loss: 7.1747\n",
      "Epoch [42/100], step[1300], loss: 7.3039\n",
      "Epoch [42/100], step[1400], loss: 7.3033\n",
      "Train Accuracy 32/45438 (0.0007)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [43/100], step[100], loss: 7.1320\n",
      "Epoch [43/100], step[200], loss: 7.2939\n",
      "Epoch [43/100], step[300], loss: 7.2504\n",
      "Epoch [43/100], step[400], loss: 7.2433\n",
      "Epoch [43/100], step[500], loss: 7.3109\n",
      "Epoch [43/100], step[600], loss: 7.2870\n",
      "Epoch [43/100], step[700], loss: 7.2935\n",
      "Epoch [43/100], step[800], loss: 7.1719\n",
      "Epoch [43/100], step[900], loss: 7.2913\n",
      "Epoch [43/100], step[1000], loss: 7.2408\n",
      "Epoch [43/100], step[1100], loss: 7.2068\n",
      "Epoch [43/100], step[1200], loss: 7.2653\n",
      "Epoch [43/100], step[1300], loss: 7.3089\n",
      "Epoch [43/100], step[1400], loss: 7.3261\n",
      "Train Accuracy 32/45438 (0.0007)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [44/100], step[100], loss: 7.2337\n",
      "Epoch [44/100], step[200], loss: 7.3119\n",
      "Epoch [44/100], step[300], loss: 7.2984\n",
      "Epoch [44/100], step[400], loss: 7.2031\n",
      "Epoch [44/100], step[500], loss: 7.2814\n",
      "Epoch [44/100], step[600], loss: 7.1621\n",
      "Epoch [44/100], step[700], loss: 7.1762\n",
      "Epoch [44/100], step[800], loss: 7.2860\n",
      "Epoch [44/100], step[900], loss: 7.3066\n",
      "Epoch [44/100], step[1000], loss: 7.2121\n",
      "Epoch [44/100], step[1100], loss: 7.3339\n",
      "Epoch [44/100], step[1200], loss: 7.3005\n",
      "Epoch [44/100], step[1300], loss: 7.3242\n",
      "Epoch [44/100], step[1400], loss: 7.3117\n",
      "Train Accuracy 32/45438 (0.0007)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [45/100], step[100], loss: 7.3836\n",
      "Epoch [45/100], step[200], loss: 7.2005\n",
      "Epoch [45/100], step[300], loss: 7.2413\n",
      "Epoch [45/100], step[400], loss: 7.3631\n",
      "Epoch [45/100], step[500], loss: 7.2410\n",
      "Epoch [45/100], step[600], loss: 7.3293\n",
      "Epoch [45/100], step[700], loss: 7.1835\n",
      "Epoch [45/100], step[800], loss: 7.1855\n",
      "Epoch [45/100], step[900], loss: 7.2619\n",
      "Epoch [45/100], step[1000], loss: 7.3134\n",
      "Epoch [45/100], step[1100], loss: 7.2516\n",
      "Epoch [45/100], step[1200], loss: 7.2163\n",
      "Epoch [45/100], step[1300], loss: 7.3214\n",
      "Epoch [45/100], step[1400], loss: 7.3191\n",
      "Train Accuracy 32/45438 (0.0007)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [46/100], step[100], loss: 7.3263\n",
      "Epoch [46/100], step[200], loss: 7.2817\n",
      "Epoch [46/100], step[300], loss: 7.2761\n",
      "Epoch [46/100], step[400], loss: 7.1969\n",
      "Epoch [46/100], step[500], loss: 7.3119\n",
      "Epoch [46/100], step[600], loss: 7.1842\n",
      "Epoch [46/100], step[700], loss: 7.2247\n",
      "Epoch [46/100], step[800], loss: 7.2378\n",
      "Epoch [46/100], step[900], loss: 7.2489\n",
      "Epoch [46/100], step[1000], loss: 7.2634\n",
      "Epoch [46/100], step[1100], loss: 7.2318\n",
      "Epoch [46/100], step[1200], loss: 7.2160\n",
      "Epoch [46/100], step[1300], loss: 7.2829\n",
      "Epoch [46/100], step[1400], loss: 7.3022\n",
      "Train Accuracy 32/45438 (0.0007)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Performance hasn't improved for 5 epochs. Adding a new cascade layer.\n",
      "Epoch [47/100], step[100], loss: 7.2809\n",
      "Epoch [47/100], step[200], loss: 7.2814\n",
      "Epoch [47/100], step[300], loss: 7.2517\n",
      "Epoch [47/100], step[400], loss: 7.2592\n",
      "Epoch [47/100], step[500], loss: 7.3116\n",
      "Epoch [47/100], step[600], loss: 7.2382\n",
      "Epoch [47/100], step[700], loss: 7.3734\n",
      "Epoch [47/100], step[800], loss: 7.2536\n",
      "Epoch [47/100], step[900], loss: 7.2046\n",
      "Epoch [47/100], step[1000], loss: 7.3115\n",
      "Epoch [47/100], step[1100], loss: 7.2192\n",
      "Epoch [47/100], step[1200], loss: 7.3467\n",
      "Epoch [47/100], step[1300], loss: 7.3227\n",
      "Epoch [47/100], step[1400], loss: 7.2245\n",
      "Train Accuracy 47/45438 (0.0010)\n",
      "Test Accuracy 17/45438 (0.0011)\n",
      "Epoch [48/100], step[100], loss: 7.2067\n",
      "Epoch [48/100], step[200], loss: 7.2417\n",
      "Epoch [48/100], step[300], loss: 7.2016\n",
      "Epoch [48/100], step[400], loss: 7.4023\n",
      "Epoch [48/100], step[500], loss: 7.2556\n",
      "Epoch [48/100], step[600], loss: 7.3037\n",
      "Epoch [48/100], step[700], loss: 7.3248\n",
      "Epoch [48/100], step[800], loss: 7.2406\n",
      "Epoch [48/100], step[900], loss: 7.2549\n",
      "Epoch [48/100], step[1000], loss: 7.3014\n",
      "Epoch [48/100], step[1100], loss: 7.1521\n",
      "Epoch [48/100], step[1200], loss: 7.2565\n",
      "Epoch [48/100], step[1300], loss: 7.2363\n",
      "Epoch [48/100], step[1400], loss: 7.3663\n",
      "Train Accuracy 47/45438 (0.0010)\n",
      "Test Accuracy 17/45438 (0.0011)\n",
      "Epoch [49/100], step[100], loss: 7.2405\n",
      "Epoch [49/100], step[200], loss: 7.3653\n",
      "Epoch [49/100], step[300], loss: 7.3001\n",
      "Epoch [49/100], step[400], loss: 7.2755\n",
      "Epoch [49/100], step[500], loss: 7.2459\n",
      "Epoch [49/100], step[600], loss: 7.2669\n",
      "Epoch [49/100], step[700], loss: 7.1928\n",
      "Epoch [49/100], step[800], loss: 7.2892\n",
      "Epoch [49/100], step[900], loss: 7.2462\n",
      "Epoch [49/100], step[1000], loss: 7.2583\n",
      "Epoch [49/100], step[1100], loss: 7.2128\n",
      "Epoch [49/100], step[1200], loss: 7.2933\n",
      "Epoch [49/100], step[1300], loss: 7.3109\n",
      "Epoch [49/100], step[1400], loss: 7.1971\n",
      "Train Accuracy 47/45438 (0.0010)\n",
      "Test Accuracy 17/45438 (0.0011)\n",
      "Epoch [50/100], step[100], loss: 7.3280\n",
      "Epoch [50/100], step[200], loss: 7.2659\n",
      "Epoch [50/100], step[300], loss: 7.3655\n",
      "Epoch [50/100], step[400], loss: 7.2975\n",
      "Epoch [50/100], step[500], loss: 7.3693\n",
      "Epoch [50/100], step[600], loss: 7.2987\n",
      "Epoch [50/100], step[700], loss: 7.2377\n",
      "Epoch [50/100], step[800], loss: 7.2878\n",
      "Epoch [50/100], step[900], loss: 7.2172\n",
      "Epoch [50/100], step[1000], loss: 7.2234\n",
      "Epoch [50/100], step[1100], loss: 7.2759\n",
      "Epoch [50/100], step[1200], loss: 7.3210\n",
      "Epoch [50/100], step[1300], loss: 7.1883\n",
      "Epoch [50/100], step[1400], loss: 7.2081\n",
      "Train Accuracy 47/45438 (0.0010)\n",
      "Test Accuracy 17/45438 (0.0011)\n",
      "Epoch [51/100], step[100], loss: 7.2832\n",
      "Epoch [51/100], step[200], loss: 7.1914\n",
      "Epoch [51/100], step[300], loss: 7.2544\n",
      "Epoch [51/100], step[400], loss: 7.2838\n",
      "Epoch [51/100], step[500], loss: 7.2981\n",
      "Epoch [51/100], step[600], loss: 7.2929\n",
      "Epoch [51/100], step[700], loss: 7.2753\n",
      "Epoch [51/100], step[800], loss: 7.2342\n",
      "Epoch [51/100], step[900], loss: 7.2230\n",
      "Epoch [51/100], step[1000], loss: 7.2682\n",
      "Epoch [51/100], step[1100], loss: 7.2445\n",
      "Epoch [51/100], step[1200], loss: 7.2665\n",
      "Epoch [51/100], step[1300], loss: 7.2103\n",
      "Epoch [51/100], step[1400], loss: 7.1568\n",
      "Train Accuracy 47/45438 (0.0010)\n",
      "Test Accuracy 17/45438 (0.0011)\n",
      "Performance hasn't improved for 5 epochs. Adding a new cascade layer.\n",
      "Epoch [52/100], step[100], loss: 7.3002\n",
      "Epoch [52/100], step[200], loss: 7.1773\n",
      "Epoch [52/100], step[300], loss: 7.2728\n",
      "Epoch [52/100], step[400], loss: 7.2495\n",
      "Epoch [52/100], step[500], loss: 7.2715\n",
      "Epoch [52/100], step[600], loss: 7.2696\n",
      "Epoch [52/100], step[700], loss: 7.3036\n",
      "Epoch [52/100], step[800], loss: 7.2147\n",
      "Epoch [52/100], step[900], loss: 7.2983\n",
      "Epoch [52/100], step[1000], loss: 7.3020\n",
      "Epoch [52/100], step[1100], loss: 7.2533\n",
      "Epoch [52/100], step[1200], loss: 7.3047\n",
      "Epoch [52/100], step[1300], loss: 7.3442\n",
      "Epoch [52/100], step[1400], loss: 7.2234\n",
      "Train Accuracy 34/45438 (0.0007)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [53/100], step[100], loss: 7.1726\n",
      "Epoch [53/100], step[200], loss: 7.2823\n",
      "Epoch [53/100], step[300], loss: 7.1651\n",
      "Epoch [53/100], step[400], loss: 7.3858\n",
      "Epoch [53/100], step[500], loss: 7.2631\n",
      "Epoch [53/100], step[600], loss: 7.2385\n",
      "Epoch [53/100], step[700], loss: 7.2336\n",
      "Epoch [53/100], step[800], loss: 7.2003\n",
      "Epoch [53/100], step[900], loss: 7.2536\n",
      "Epoch [53/100], step[1000], loss: 7.2299\n",
      "Epoch [53/100], step[1100], loss: 7.2177\n",
      "Epoch [53/100], step[1200], loss: 7.2686\n",
      "Epoch [53/100], step[1300], loss: 7.2173\n",
      "Epoch [53/100], step[1400], loss: 7.1632\n",
      "Train Accuracy 34/45438 (0.0007)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [54/100], step[100], loss: 7.2455\n",
      "Epoch [54/100], step[200], loss: 7.2668\n",
      "Epoch [54/100], step[300], loss: 7.1598\n",
      "Epoch [54/100], step[400], loss: 7.2522\n",
      "Epoch [54/100], step[500], loss: 7.2633\n",
      "Epoch [54/100], step[600], loss: 7.2845\n",
      "Epoch [54/100], step[700], loss: 7.2276\n",
      "Epoch [54/100], step[800], loss: 7.1748\n",
      "Epoch [54/100], step[900], loss: 7.2192\n",
      "Epoch [54/100], step[1000], loss: 7.2768\n",
      "Epoch [54/100], step[1100], loss: 7.2619\n",
      "Epoch [54/100], step[1200], loss: 7.1743\n",
      "Epoch [54/100], step[1300], loss: 7.2078\n",
      "Epoch [54/100], step[1400], loss: 7.3082\n",
      "Train Accuracy 34/45438 (0.0007)\n",
      "Test Accuracy 8/45438 (0.0005)\n",
      "Epoch [55/100], step[100], loss: 7.2613\n",
      "Epoch [55/100], step[200], loss: 7.2019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 11\u001b[0m     train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m test()\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(train_accuracy \u001b[38;5;241m>\u001b[39m best_accuracy):\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 16\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], step[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\rprop.py:106\u001b[0m, in \u001b[0;36mRprop.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    102\u001b[0m     maximize \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params, grads, prevs, step_sizes)\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mrprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprevs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43metaminus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaminus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43metaplus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaplus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\rprop.py:205\u001b[0m, in \u001b[0;36mrprop\u001b[1;34m(params, grads, prevs, step_sizes, foreach, maximize, differentiable, step_size_min, step_size_max, etaminus, etaplus)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_rprop\n\u001b[1;32m--> 205\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprevs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43metaminus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaminus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43metaplus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaplus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\rprop.py:300\u001b[0m, in \u001b[0;36m_multi_tensor_rprop\u001b[1;34m(params, grads, prevs, step_sizes, step_size_min, step_size_max, etaminus, etaplus, maximize, differentiable)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maximize:\n\u001b[0;32m    298\u001b[0m     grouped_grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_neg(grouped_grads)\n\u001b[1;32m--> 300\u001b[0m signs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrouped_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouped_prevs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m signs \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39msign() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m signs]\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sign \u001b[38;5;129;01min\u001b[39;00m signs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "criterion = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer = optim.Rprop(model.parameters())\n",
    "\n",
    "best_accuracy = 0.0\n",
    "epochs_without_improvement = 0\n",
    "patience = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_accuracy = train()\n",
    "    test_accuracy = test()\n",
    "\n",
    "    if(train_accuracy > best_accuracy):\n",
    "        best_accuracy = train_accuracy\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # If performance hasn't improved for 'patience' epochs, add a layer\n",
    "    if(epochs_without_improvement == patience):\n",
    "        print(\"Performance hasn't improved for {} epochs. Adding a new cascade layer.\".format(patience))\n",
    "        model.add_cascade_layer()\n",
    "        epochs_without_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efb2eb-a57f-492d-bff7-0810d991f440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea711e4b-53ee-488b-92ce-25feb0791a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77731fd9-04f7-476e-8bbd-3f6903582776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
